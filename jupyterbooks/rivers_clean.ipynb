{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import shapely.geometry\n",
    "import shapely.ops\n",
    "import pandas as pd\n",
    "import folium\n",
    "import earthpy\n",
    "from ipyleaflet import Map, GeoData, basemaps, LayersControl\n",
    "import json\n",
    "from ipywidgets import HTML\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import time\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buffering parks and clipping rivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parks = gpd.read_file(\"../../data/rivers/Polygon_layer.shp\").to_crs(epsg=4269)\n",
    "\n",
    "\n",
    "for index, park in parks.iterrows():\n",
    "    print(park.loc['name'])\n",
    "\n",
    "\n",
    "park_name = 'Majete Wildlife Reserve'\n",
    "\n",
    "\n",
    "# Buffer\n",
    "park_buffer = parks.loc[parks['name'] == park_name].to_crs('+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs ')\n",
    "\n",
    "park_buffer['geometry'] = park_buffer['geometry'].geometry.buffer(500000)\n",
    "park_buffer = park_buffer.to_crs(epsg=4269)\n",
    "\n",
    "# Park\n",
    "park = parks.loc[parks['name'] == park_name].to_crs('+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs ')\n",
    "park['geometry'] = park['geometry'].geometry.buffer(500)\n",
    "\n",
    "park = park.to_crs(epsg=4269)\n",
    "\n",
    "# Rivers\n",
    "try:\n",
    "    rwa_rivers\n",
    "except NameError:\n",
    "    rwa_rivers = gpd.read_file(\"../../data/rivers/clipped_rivers.shp\").to_crs(epsg=4269)\n",
    "\n",
    "# Buffer rivers\n",
    "buffer_border = gpd.GeoDataFrame(geometry=park_buffer.boundary)\n",
    "buffer_rivers = gpd.sjoin(rwa_rivers, park_buffer, how='inner', predicate='within')\n",
    "\n",
    "# Park rivers\n",
    "park_rivers = gpd.sjoin(rwa_rivers, park, how='inner', predicate='within')\n",
    "\n",
    "park_rivers = park_rivers.reset_index(drop=True)\n",
    "\n",
    "# Set property if in park\n",
    "buffer_rivers['inpark_par'] = 0\n",
    "\n",
    "for index1, river1 in buffer_rivers.iterrows():\n",
    "    if river1['NOID'] in park_rivers['NOID'].values:\n",
    "        buffer_rivers.at[index1, 'inpark_par'] = 1\n",
    "        \n",
    "buffer_rivers = buffer_rivers.rename(columns={'index_left': 'i_l'})\n",
    "buffer_rivers = buffer_rivers.rename(columns={'index_right': 'i_r'})\n",
    "buffer_rivers = buffer_rivers.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Rivers entering the park from outside\n",
    "buffer_rivers['o_outside_dis']=0.0\n",
    "buffer_rivers['o_inflow_par']=0\n",
    "buffer_rivers['o_through_par']=0\n",
    "buffer_rivers['o_park_dis']=0.0 \n",
    "\n",
    "# Rivers starting in park\n",
    "buffer_rivers['i_start_dis']=0.0\n",
    "buffer_rivers['i_park_dis']=0.0 \n",
    "buffer_rivers['i_park_fra']=0.0 \n",
    "\n",
    "# All rivers\n",
    "buffer_rivers['park_dis']=0.0\n",
    "buffer_rivers['intersect_par'] = 0\n",
    "buffer_rivers['park_fra']=0.0 \n",
    "\n",
    "# Rivers on boundary of park\n",
    "park_rivers['intersect_par'] = 0\n",
    "park_rivers['o_outside_dis']=0.0\n",
    "buffer_rivers['o_park_fra']=0.0 \n",
    "\n",
    "\n",
    "park_border = gpd.GeoDataFrame(geometry=park.boundary)\n",
    "all_inters = gpd.sjoin(buffer_rivers, park_border, op='intersects')\n",
    "\n",
    "\n",
    "\n",
    "for index1, river1 in buffer_rivers.iterrows():\n",
    "    if river1['NOID'] in all_inters['NOID'].values:\n",
    "        buffer_rivers.at[index1, 'intersect_par'] = 1\n",
    "        buffer_rivers.at[index1, 'inpark_par'] = 1\n",
    "\n",
    "for index1, river1 in buffer_rivers.iterrows():\n",
    "    if river1['NOID'] in park_rivers['NOID'].values:\n",
    "        buffer_rivers.at[index1, 'inpark_par'] = 1\n",
    "        \n",
    "\n",
    "for index1, river1 in all_inters.iterrows():\n",
    "    if river1['NOID'] in park_rivers['NOID'].values:\n",
    "        park_rivers.at[index1, 'intersect_par'] = 1  \n",
    "        \n",
    "\n",
    "all_inters = all_inters.reset_index(drop=True)        \n",
    "inters = gpd.GeoDataFrame()\n",
    "for index1, r1 in all_inters.iterrows():\n",
    "    already_intersected = 0\n",
    "    for index2, r2 in all_inters.iterrows():\n",
    "        if r1['NOID'] == r2['NDOID']:\n",
    "            already_intersected = 1\n",
    "    if already_intersected != 1:\n",
    "        inters = inters.append(all_inters.iloc[index1])\n",
    "            \n",
    "# Make df of all rivers inside and intersecting park\n",
    "in_park = park_rivers.append(inters)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "buffer_rivers.plot(ax=ax, color='grey')\n",
    "park_border.plot(ax=ax, color='red')\n",
    "in_park.plot(ax=ax, color='orange')\n",
    "\n",
    "# inters.plot(ax=ax, color='blue')\n",
    "plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find upstream river outside park"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "in_inters = gpd.GeoDataFrame()\n",
    "inters = inters.reset_index(drop=True)\n",
    "\n",
    "# Loop through all rivers intersecting with park border\n",
    "for index1, r1 in inters.iterrows():\n",
    "    # All rivers starting\n",
    "    if r1['NUOID'] is None:\n",
    "        pass\n",
    "    # All rivers not starting\n",
    "    else:\n",
    "        for index2, r2 in buffer_rivers.loc[buffer_rivers['NDOID']==r1['NOID']].iterrows():\n",
    "            # If river (r2) flows into river on border (r1)\n",
    "            if r1['NOID'] == r2['NDOID']:\n",
    "                # to make sure upstream river (r2) is outside the park\n",
    "                if r2['inpark_par'] != 1:\n",
    "                    # then upstream river (r2) is flowing into the park\n",
    "                    inters.at[index1, 'o_inflow_par'] = 1\n",
    "                    in_inters = in_inters.append(inters.iloc[index1])\n",
    "\n",
    "print(inters.loc[inters['o_inflow_par']==1,'NOID'])                   \n",
    "\n",
    "# Set the o_inflow_par of buffer_rivers also to 1\n",
    "buffer_rivers.loc[buffer_rivers['NOID'].isin(inters.loc[inters['o_inflow_par']==1,'NOID']), 'o_inflow_par'] = 1\n",
    "\n",
    "# If there are rivers flowing into the park\n",
    "if in_inters.shape[0] > 0:                \n",
    "\n",
    "    # drop all duplicate in-flowing rivers\n",
    "    in_inters = in_inters.drop_duplicates()\n",
    "    print('Number of inflowing rivers: ' + str(len(in_inters)))\n",
    "\n",
    "    # set property on buffer_rivers to know that river is entering the park from outside\n",
    "# may be REDUNDANT\n",
    "    for index1, river1 in buffer_rivers.iterrows():\n",
    "        if river1['NOID'] in in_inters['NOID'].values:\n",
    "            buffer_rivers.at[index1, 'o_inflow_par'] = 1  \n",
    "\n",
    "    # plot to show in-flowing rivers\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_aspect('equal')\n",
    "    park_border.plot(ax=ax, color='grey')\n",
    "    park_rivers.plot(ax=ax, color='black')\n",
    "    inters.plot(ax=ax, color='blue')\n",
    "    in_inters.plot(ax=ax, color='red')\n",
    "    plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating outside flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if in_inters.shape[0] > 0:                \n",
    "\n",
    "    for index1, r1 in in_inters.iterrows(): \n",
    "        print(\"\\n\")\n",
    "        print(r1['NOID'])\n",
    "        print('start_flow: ' + str(r1['DIS_AV_CMS'] ))\n",
    "\n",
    "\n",
    "        run = 0\n",
    "        # If the river has reached the park boundary and is now flowing out\n",
    "        out_flow = 0\n",
    "        new_inters = gpd.GeoDataFrame()\n",
    "\n",
    "        # While the river is still inside the park\n",
    "        while out_flow == 0:\n",
    "            run += 1\n",
    "            print('run' + str(run))\n",
    "            # If in-flowing rivers on park boundary\n",
    "            if run == 1:\n",
    "                # For the next downstream rivers\n",
    "                for index2, r2 in buffer_rivers.loc[buffer_rivers['NOID']==r1['NDOID']].iterrows():\n",
    "                    # If inflowing river (r1) is flowing into downstream river (r2)\n",
    "                    if r1['NDOID'] == r2['NOID']:\n",
    "\n",
    "                        # Set the downstream river (r2)'s outside flow to the inflowing river (r1)'s flow\n",
    "                        buffer_rivers.at[index2, 'o_outside_dis'] = r2['o_outside_dis']  + r1['DIS_AV_CMS']\n",
    "                        \n",
    "                        print('o_outside_dis: ' + str(buffer_rivers.at[index2, 'o_outside_dis']))\n",
    "                        \n",
    "                        # Add r2 to become the next round's upstream river\n",
    "                        new_inters = new_inters.append(buffer_rivers.iloc[index2])\n",
    "                        \n",
    "                        # Check if r2 is already crossing the park boundary (leaving the park)\n",
    "                        if r2['inpark_par'] == 0:\n",
    "                            out_flow = 1\n",
    "                        else:\n",
    "                            buffer_rivers.at[index2, 'o_through_par'] = 1\n",
    "                            \n",
    "\n",
    "            # If not the in-flowing river, but a next-in-line downstream river\n",
    "\n",
    "            else:\n",
    "                for index3, r3 in new_inters.iterrows():\n",
    "                    new_inters = gpd.GeoDataFrame()\n",
    "                    for index4, r4 in buffer_rivers.loc[buffer_rivers['NOID']==r3['NDOID']].iterrows():\n",
    "                        # If upstream river (r3) is flowing into downstream river (r4)                    \n",
    "                        if r3['NDOID'] == r4['NOID']:\n",
    "                            \n",
    "                            # Set the downstream river (r4)'s outside flow to the inflowing river (r3)'s flow\n",
    "                            buffer_rivers.at[index4, 'o_outside_dis'] = r4['o_outside_dis']  + r1['DIS_AV_CMS']\n",
    "\n",
    "                            print('o_outside_dis: ' + str(buffer_rivers.at[index4, 'o_outside_dis']))\n",
    "                                                        \n",
    "                            # Add r4 to become the next round's upstream river\n",
    "                            new_inters = new_inters.append(buffer_rivers.iloc[index4])\n",
    "                            \n",
    "                            # Check if r4 is already crossing the park boundary (leaving the park)\n",
    "                            if r4['inpark_par'] == 0:\n",
    "                                out_flow = 1\n",
    "                                \n",
    "                            else:\n",
    "                                buffer_rivers.at[index4, 'o_through_par'] = 1\n",
    "                                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate park contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for index1, r1 in buffer_rivers.iterrows():\n",
    "    if (r1['o_outside_dis'] != 0) and r1['inpark_par'] == 1:\n",
    "        buffer_rivers.at[index1, 'o_park_dis'] = r1['DIS_AV_CMS'] - r1['o_outside_dis']\n",
    "        buffer_rivers.at[index1, 'o_park_fra'] = buffer_rivers.at[index1, 'o_park_dis']/r1['DIS_AV_CMS']\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine with rivers starting inside park"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Loop through all rivers intersecting with park border\n",
    "            \n",
    "new_inters = inters\n",
    "print('No of inters:' + str(inters.shape[0]))\n",
    "runs = 500\n",
    "run = 0\n",
    "while run < runs:\n",
    "    run += 1\n",
    "    print(round(100*run/runs))\n",
    "    \n",
    "    new_new_inters = new_inters\n",
    "    new_inters = gpd.GeoDataFrame()\n",
    "    \n",
    "    for index1, r1 in new_new_inters.iterrows():\n",
    "        \n",
    "        \n",
    "        # RIVERS STARTING IN PARK\n",
    "\n",
    "        # If river originates in park, and not flowing from the outside in.\n",
    "        if run == 1 and r1['o_inflow_par'] == 0:\n",
    "            # Set its own i_start_dis to its dis\n",
    "            buffer_rivers.loc[buffer_rivers['NOID']==r1['NOID'],'i_start_dis'] = r1['DIS_AV_CMS'] + r1['i_start_dis']\n",
    "\n",
    "\n",
    "        # If the river is downstream from another\n",
    "        else:\n",
    "            # Subset all rivers upstream from r1\n",
    "            upstream_ids1 = map(int, r1['NUOID'].split('_'))\n",
    "            upstream1 = buffer_rivers.loc[buffer_rivers['NOID'].isin(upstream_ids1)].reset_index()\n",
    "\n",
    "\n",
    "            # r1's i_start_dis will be equal to the sum of all the other's i_start_dis\n",
    "            buffer_rivers.at[index1, 'i_start_dis'] = upstream1['i_start_dis'].sum()\n",
    "            \n",
    "            if (r1['o_through_par'] == 0) and (r2['intersect_par'] == 0):\n",
    "                buffer_rivers.at[index1, 'o_park_dis'] = upstream1['o_park_dis'].sum()\n",
    "                buffer_rivers.at[index1, 'o_park_fra'] = upstream1['o_park_fra'].sum()\n",
    "                          \n",
    "\n",
    "            # POPULATE ALL RIVERS DOWNSTREAM\n",
    "\n",
    "        for index2, r2 in buffer_rivers.loc[buffer_rivers['NOID']==r1['NDOID']].iterrows():\n",
    "            \n",
    "            if (r1['NDOID'] == r2['NOID']):\n",
    "                new_inters = new_inters.append(buffer_rivers.iloc[index2])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "buffer_rivers['i_park_fra'] = buffer_rivers['i_start_dis']/buffer_rivers['DIS_AV_CMS']\n",
    "\n",
    "buffer_rivers['park_dis'] = buffer_rivers['o_park_dis'] + buffer_rivers['i_start_dis']\n",
    "\n",
    "buffer_rivers['park_fra'] = buffer_rivers['park_dis']/buffer_rivers['DIS_AV_CMS']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for index1, r1 in buffer_rivers.iterrows():\n",
    "    if buffer_rivers.at[index1, 'inpark_par'] == 1 and buffer_rivers.at[index1, 'o_through_par'] != 1 and buffer_rivers.at[index1, 'o_inflow_par'] == 0:\n",
    "        buffer_rivers.at[index1, 'park_fra'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "relevant_rivers = buffer_rivers.loc[(buffer_rivers['park_fra'] > 0) | (buffer_rivers['inpark_par'] == 1)]\n",
    "m = folium.Map([park.centroid.y, park.centroid.x],\n",
    "                  zoom_start=10,\n",
    "                  tiles='Stamen Terrain')\n",
    "\n",
    "def create_cols(dis):\n",
    "    if dis < 0.1:\n",
    "        col = '#3e5946'\n",
    "    else:\n",
    "        col = '#34278f'\n",
    "    return col\n",
    "\n",
    "relevant_rivers['col'] = pd.cut(relevant_rivers['park_fra'], bins=5, labels=['white', '#73a4af', '#4f84a2', '#253c5e', '#182749'])\n",
    "\n",
    "\n",
    "gj = folium.GeoJson(\n",
    "    relevant_rivers,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': feature['properties']['col'],\n",
    "        'color' : feature['properties']['col'],\n",
    "        'weight' : 3,\n",
    "        'fillOpacity' : 0.5,\n",
    "        },\n",
    "    popup=folium.GeoJsonPopup(fields=[ 'DIS_AV_CMS',\n",
    "        'NOID', 'NDOID', 'NUOID', 'o_inflow_par', 'o_through_par', 'inpark_par', 'o_outside_dis',\n",
    "        'o_park_dis', 'o_park_fra', 'i_start_dis', 'i_park_fra', 'park_fra', 'park_dis'])\n",
    "    )\n",
    "\n",
    "park_plot = folium.GeoJson(\n",
    "    park,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': 'red',\n",
    "        'color' : 'red',\n",
    "        'weight' : 3,\n",
    "        'fillOpacity' : 0.5,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "park_plot.add_to(m)\n",
    "gj.add_to(m)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
